{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-df05970161c6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-df05970161c6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Royen Clément\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Royen Clément\n",
    "12/04/2016\n",
    "MACHINES À VECTEURS SUPPORT\n",
    "- Approche intuitive -\n",
    "\n",
    "%run 'svmgui.py'\n",
    "Question 1\n",
    "Observons le cas d'une distribution unimodale\n",
    "Lorsque les classes sont séparables il n'y a aucun impact en changeant la valeur de C sur la frontière ou sur sa marge avec le classifieur linéaire. Quant au classifieur à noyau gaussien, l'impact s'observe surtout sur les marges qui ont tendance à s'élargir lorsque C est grand et à overfitter lorsque C est trop petit. Le modèle linéaire est le plus adapté pour un problème de classification à donnée séparable. Lorsque l'on augmente γ les marges tendent à disparaître et la frontière vient overfitter une des classes.\n",
    "Lorsqu'il y a recouvrement des classes, plus C est bas plus les marges sont étroites sur le modèle linéaire mais la frontière ne semble pas se modifier sensiblement sauf lorsque C est proche de 0 dans quel cas le classifieur ne distingue aucune classe. Pour le classifieur à noyau gaussien on observe le même phénomène que pour le cas des données séparables mis à part le fait que la frontière entoure tantôt une classe tantôt l'autre. Lorsque l'on fait tendre gamma vers zéro cette frontière tend à s'éclater et le classifieur tend vers le classifieur linéaire (on peut s'en convaincre en effectuant un développement limité à l'ordre 1 de l'exponentielle).\n",
    "Le choix des paramètres peut être crucial lorsqu'il y a recouvrement des classes. Car il faut séparer les classes tout en évitant l'overfitting.\n",
    "Question 2\n",
    "Les trois zones d'intérêt sont:\n",
    "La zone intermarge hors la zone à forte densité de population où l'ajout d'un d'un point, quelque soit sa classe, tend à déplacer ou rapprocher la frontière vers ce point tout en élargissant la marge, si le point n'est pas dans sa zone estimée, et en la contractant dans le cas contraire. (le αi associé à ce point est non nul, c'est un vecteur de support).\n",
    "Le demi plan contenant les points de la classe blanche et noire où dans le premier l'ajout d'un point blanc n'a aucun effet sur le classifieur (αi=0 ou presque) et l'ajout d'un point bleu rapproche la frontière et/ou élargit la marge (αi≠0). De même pour le deuxième demi plan en inversant le rôle des classes.\n",
    "La zone de recouvrement qui n'a aucun effet sur le classifieur (αi=0 ou presque) à moins de rajouter une quantité considérable de nouveaux points.\n",
    "In [1]:\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='recouvrement.png')\n",
    "\n",
    "\n",
    "Question 3\n",
    "\n",
    "\n",
    "Image(filename='C0_001.png') ## Cas C=0.001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Image(filename='C0_0001.png') ## Cas C=0.0001\n",
    "\n",
    "\n",
    "Comme on peut s'y attendre lorsque l'on diminue C la marge sépart beaucoup mieux du côté où la population est plus nombreuse (ici la classe blanche) que de l'autre (classe noire). Ainsi dans notre exemple la majorité des points de classe noire sont dans l'espace intermarge alors qu'une minorité de points de classe blanche s'y retrouve. Si on continue de diminuer C ou si on étudie le cas où il y a recouvrement des classes, la frontière mettra clairement en défaut la classe noire et très peu de donnée de cette classe seront séparées de la classe blanche. Il faut donc repondérer l'attache aux données sur la classe moins présente pour que celle-ci puisse être aussi bien séparée que la classe majoritaire, c'est-à-dire proportionnellement à son effectif total, de sorte que le déséquilibre joue moins sur le pouvoir de séparation des classes.\n",
    "- Classification -\n",
    "Question 4\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  #données\n",
    "Y = iris.target #classes\n",
    "X = X[Y != 0, :2] #on restreint à 2 features et à 2 classes\n",
    "Y = Y[Y != 0]#On restreint à 2 classes: 1 et 2\n",
    "# classifieur linéaire\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X,Y)\n",
    "# plot\n",
    "plt.figure(1)\n",
    "plot_2d(X,Y)\n",
    "frontiere(clf.predict,X,step=50)\n",
    "plt.axis('tight')\n",
    "plt.title('Frontiere de decision')\n",
    "\n",
    "\n",
    "\n",
    "On remarque qu'il y a recouvrement des classes et que par conséquent le classifieur n'arrive pas bien à séparer les classes du training set dans le plan.\n",
    "Affichons le score moyen:\n",
    "\n",
    "\n",
    "print('Score moyen : %d %%'%(clf.score(X,Y)*100))\n",
    "Score moyen : 73 %\n",
    "On obtient alors un score de 73% pour C=1 sur les données d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
