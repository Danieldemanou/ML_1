{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : MiniNN Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 décembre 2016  \n",
    "Adapté du TP de Gaétan Marceau-Caron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle _Softmax_ (aussi connu sous le nom de MaxEnt) pour traiter le problème de classification probabiliste.\n",
    "Le but était de présenter deux étapes importantes de l'entraînement: la {\\it forward propagation} et la mise à jour des paramètres.\n",
    "Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachées.\n",
    "\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrée des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "D'un point de vue computationnel, un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres.\n",
    "Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. \n",
    "En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "La thèse du {\\it Deep Learning} nous dit que les modules près des données d'entrée doivent apprennent des features de bas niveau, e.g., filtre de Gabor pour l'image, alors que les modules près de la sortie apprennent des features de haut niveau, e.g., la probabilité qu'il y ait un chat dans l'image.\n",
    "A priori, cette hiéarchie des features n'est pas imposée par le programmeur, mais apparaît naturellement lors de l'entraînement avec l'algorithme de backpropagation .\n",
    "\n",
    "Le but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones: la forward propagation, la backpropagation et la mise à jour des paramètres.\n",
    "Ensuite, vous pouvez créer un test permettant de vérifier l'implémentation: le test des différences finies.\n",
    "Finalement, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs : \n",
    "    1. Télécharger le TP2\n",
    "    2. Implementer la fonction forward (nn ops.py:72)\n",
    "    3. Implementer la fonction sigmoid et sa d ́eriv ́ee (nn ops.py:149)\n",
    "    4. Implementer la fonction backward (nn ops.py:93)\n",
    "    5. Implementer la fonction update (nn ops.py:123)\n",
    "    6. Entrainer miniNN et obtenir une accuracy meilleur que le modèle softmax (de 0.92)\n",
    "\n",
    "\n",
    "Et optionellemnt : \n",
    "    7. Implementer le test des diff ́erences finies (fd test.py)\n",
    "    \n",
    "La différence finie est une approximation de la dérivée partielle:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l(w)}{\\partial w_{i}} \\approx \\frac{l(w + \\epsilon e_{i}) - l(w - \\epsilon e_{i})}{2 \\epsilon}\n",
    "\\end{equation}\n",
    "où $l$ est une fonction à plusieurs variables avec ses dérivées partielles définies, $w$ est un vecteur, $w_{i}$ est sa $i$ème composante, $\\epsilon$ est la longeur du pas et $e_{i}$ est le $i$ème vecteur de la base canonique de l'espace euclidien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable\n",
    "\n",
    "- __Date du livrable__: Le 16 décembre 2015  \n",
    "- __Format du livrable__ un fichier compressé nommé \"DL\\_tp2\\_prénom\\_nom.zip\" contenant le code et le résumé   \n",
    "- __Dépôt__ à l'adresse _{thomas.schmitt@inria.fr}_ avec comme objet du message \"DL\\_tp2\\_prénom\\_nom\".  \n",
    "- __Description__  \n",
    "Le livrable associé au TP2 doit contenir le code de MiniNN complété et accompagné d'un résumé de une à deux pages.\n",
    "Le code doit s'exécuter avec la commande _python miniNN.py_ et afficher l'évolution de l'apprentissage (sortie par défaut du programme).  \n",
    "( Ou bien être sous la forme d'un notebook. )\n",
    "\n",
    "Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones.\n",
    "Ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre.\n",
    "Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone de Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and minibatch (as in miniNN.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Pour ne pas reload le kernel quand un fichier .py change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math, time, sys\n",
    "import dataset_loader\n",
    "from nn_ops import *\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    print('download mnist data')\n",
    "    !curl \"http://deeplearning.net/data/mnist/mnist.pkl.gz\"  -o \"mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the arguments \n",
    "args = parseArgs_ipython(arch = [128,128], act_func = \"ReLU\", batch_size = 500, eta = .01, n_epoch = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--arch ARCH [ARCH ...]] [--act_func ACT_FUNC]\n",
      "                   [--batch_size BATCH_SIZE] [--eta ETA] [--n_epoch N_EPOCH]\n",
      "__main__.py: error: unrecognized arguments: -f /Users/DEMANOU/Library/Jupyter/runtime/kernel-51c804c7-f346-40da-843f-abccf9c35766.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DEMANOU/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %load miniNN.py\n",
    "\n",
    "\n",
    "import copy, math, time, sys\n",
    "import dataset_loader\n",
    "from nn_ops import *\n",
    "\n",
    "#############################\n",
    "### Preliminaries\n",
    "#############################\n",
    "\n",
    "# Retrieve the arguments from the command-line\n",
    "args = parseArgs()\n",
    "\n",
    "# Fix the seed for the random generator\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "#############################\n",
    "### Dataset Handling\n",
    "#############################\n",
    "\n",
    "### Load the dataset\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "### Define the dataset variables\n",
    "n_training = train_set[0].shape[0]\n",
    "n_feature = train_set[0].shape[1]\n",
    "n_label = np.max(train_set[1])+1\n",
    "\n",
    "#############################\n",
    "### Neural Network parameters\n",
    "#############################\n",
    "\n",
    "### Activation function\n",
    "act_func_name = args.act_func\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + args.arch + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W,B,act_func,nb_params = initNetwork(nn_arch,act_func_name)\n",
    "\n",
    "#############################\n",
    "### Optimization parameters\n",
    "#############################\n",
    "eta = args.eta\n",
    "batch_size = args.batch_size\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = args.n_epoch \n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "printDescription(\"Bprop\", eta, nn_arch, act_func_name, batch_size, nb_params)\n",
    "print(\"epoch time(s) train_loss train_accuracy valid_loss valid_accuracy eta\") \n",
    "\n",
    "#############################\n",
    "### Learning process\n",
    "#############################\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y,Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out-one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n",
    "\n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_accuracy) + \" \" + str(valid_loss) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "\n",
    "    print(result_line)\n",
    "    sys.stdout.flush() # Force emptying the stdout buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of the experiment\n",
      "----------\n",
      "Learning algorithm: Bprop\n",
      "Initial step-size: 0.01\n",
      "Network Architecture: [784 128 128  10]\n",
      "Number of parameters: 118282\n",
      "Minibatch size: 500\n",
      "Activation: ReLU\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Fix the seed for the random generator\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "#############################\n",
    "### Dataset Handling\n",
    "#############################\n",
    "\n",
    "### Load the dataset\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "### Define the dataset variables\n",
    "n_training = train_set[0].shape[0]\n",
    "n_feature = train_set[0].shape[1]\n",
    "n_label = np.max(train_set[1])+1\n",
    "\n",
    "#############################\n",
    "### Neural Network parameters\n",
    "#############################\n",
    "\n",
    "### Activation function\n",
    "act_func_name = args.act_func\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + args.arch + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W,B,act_func,nb_params = initNetwork(nn_arch,act_func_name)\n",
    "\n",
    "#############################\n",
    "### Optimization parameters\n",
    "#############################\n",
    "eta = args.eta\n",
    "batch_size = args.batch_size\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = args.n_epoch \n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "printDescription(\"Bprop\", eta, nn_arch, act_func_name, batch_size, nb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Mini-batch creation\n",
    "j = 0\n",
    "batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "X_bacth = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appel de chaque fonction, avec les valeurs calculé avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Forward propagation\n",
    "Y,Yp = forward(act_func, W, B, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "assert Y[0].shape == (n_feature, batch_size)\n",
    "assert Y[-1].shape == (10, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Yp[0].shape == (nn_arch[1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DEMANOU/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:32: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Compute the softmax\n",
    "out = softmax(Y[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert out.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Compute the gradient at the top layer\n",
    "derror = out-one_hot_batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert derror.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Backpropagation\n",
    "gradB = backward(derror, W, Yp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(gradB) == len(nn_arch)-1\n",
    "for gradw,dim in zip(gradB,nn_arch[1:]):\n",
    "    gradw.shape = (dim,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Update the parameters\n",
    "\n",
    "new_W, new_B = update(eta, batch_size, W, B, gradB, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "grad_w = gradB[0].dot(Y[1].T)/batch_size\n",
    "print(grad_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DEMANOU/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:32: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "### Training accuracy\n",
    "train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "### Valid accuracy\n",
    "valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099360000000000004"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099000000000000005"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DEMANOU/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:32: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.632863999999998 2.30129239395 0.11356 2.30232619269 0.1064 1\n",
      "1 10.890813000000001 2.30129242141 0.11356 2.30232622516 0.1064 1\n",
      "2 16.490642 2.30129242141 0.11356 2.30232622516 0.1064 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a01880a003e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m### Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m### Compute the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DEMANOU/Downloads/DL_tp2_Daniel_DEMANOU/nn_ops.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(act_func, W, B, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mYp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DEMANOU/Downloads/DL_tp2_Daniel_DEMANOU/nn_ops.py\u001b[0m in \u001b[0;36mReLU\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                     \u001b[0myp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "tranval = []\n",
    "testval = []\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y,Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out-one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n",
    "\n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_accuracy) + \" \" + str(valid_loss) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "    tranval.append(train_accuracy)\n",
    "    testval.append(valid_accuracy)\n",
    "    \n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10e2bafd0>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAF5CAYAAAAcQxneAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e11bb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tranval,label='train_acc')\n",
    "plt.plot(testval,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
