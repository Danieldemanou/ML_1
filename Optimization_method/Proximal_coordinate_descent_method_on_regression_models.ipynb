{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs : Proximal coordinate descent method on regression models\n",
    "\n",
    "\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- proximal coordinate descent\n",
    "\n",
    "for \n",
    "- Lasso / L1 linear regression\n",
    "- non-negative least squares (NNLS)\n",
    "\n",
    "models.\n",
    "\n",
    "The proximal operators we will use are the \n",
    "- L1 penalization\n",
    "- indicator function of $\\mathbb{R}_+$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp_cd_wamo_daniel_and_guilhem-ducléon_samuel.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"daniel\"\n",
    "ln1 = \"wamo\"\n",
    "fn2 = \"samuel\"\n",
    "ln2 = \"guilhem-ducléon\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp_cd\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to embed figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 : Introduction\n",
    "\n",
    "We'll start by generating sparse positive vectors and simulating data\n",
    "\n",
    "### Getting sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x22362b80400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223627be7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features = 50\n",
    "n_samples = 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the simulation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a linear regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape=(n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal operators and Solver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind that the proximal operator of a fonction $g$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{prox}_g(y, t) = \\arg\\min_x \\Big\\{ \\frac 12 \\|x - y\\|_2^2 + t g(x) \\Big\\}.\n",
    "$$\n",
    "\n",
    "where $t \\geq 0$ is a non-negative number.\n",
    "We have in mind to use the following cases\n",
    "\n",
    "- Lasso penalization, where $g(x) = s \\|x\\|_1$\n",
    "- Indicator function of $\\mathbb{R}_+$, where $g(x) = i_{x \\geq 0}(\\cdot)$\n",
    "\n",
    "where $s \\geq 0$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize:\n",
    "$$\n",
    "\\arg\\min_x F(x)\n",
    "$$\n",
    "with\n",
    "$$\n",
    " F(x) = \\frac{1}{2} \\|b - Ax\\|^2 + g(x)\n",
    "$$\n",
    "\n",
    "## Questions\n",
    "\n",
    "- Code a function that computes $g(x)$ and $\\text{prox}_g(x)$ for in both cases\n",
    "- Justify why proximal coordinate descent can be applied to obtain a minimum of such objective functions.\n",
    "- Starting from the code provided in the notebook presented during the coordinate descent course as well as the code below, implement a proximal coordinate method for both penalties.\n",
    "- Evaluate qualitatively the convergence when varying the conditioning of the problem.\n",
    "- Bonus: Try to show that coordinate is much less affected by bad conditioning that proximal gradient descent.\n",
    "\n",
    "### You are expected to implement the smart residuals updates !\n",
    "\n",
    "### You are very welcome to reuse everything you did for TP1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question: Code a function that computes $g(x)$ and $\\text{prox}_g(x)$ for in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prox_lasso(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the Lasso at x with strength t\"\"\"\n",
    "    ret = np.multiply(np.sign(x), np.maximum(0, (np.absolute(x) - s*t )))\n",
    "    \n",
    "    return ret\n",
    "    \n",
    "def lasso(x, s):\n",
    "    \"\"\"Value of the Lasso penalization at x with strength t\"\"\"\n",
    "    ret = s * np.linalg.norm(x, 1)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def prox_indicator(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the indicator function of R+ at x with strength t\"\"\"\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def indicator(x, s):\n",
    "    \"\"\"Value of the indicator function of R+ at x\"\"\"\n",
    "    if (x < 0).any():\n",
    "        return np.inf\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Justify why proximal coordinate descent can be applied to obtain a minimum of such objective functions.\n",
    "\n",
    "f is convex and differentiable.\n",
    "\n",
    "Moreover, the non-smooth part (g) is separable here for both functions: lasso and indicator of R+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Starting from the code provided in the notebook presented during the coordinate descent course as well as the code below, implement a proximal coordinate method for both penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def cd_linreg(x0, A, b, g, prox_g, s=0., n_iter=50,\n",
    "              x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "\n",
    "    Minimize :\n",
    "    \n",
    "    1/2 ||b−Ax||^2 + s * g(x)\n",
    "    \n",
    "    with coodinate descent.\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = 0.5 * np.linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "    objectives.append(obj)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Lauching Coordinate Descent solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "\n",
    "    r = b - A.dot(x)\n",
    "    tmp = 0\n",
    "    \n",
    "    for k in range(n_iter + 1):\n",
    "\n",
    "        x_new = x.copy()\n",
    "        \n",
    "        i = random.randint(0, n_features - 1)\n",
    "        \n",
    "        L_i = A[:, i].T.dot(A[:, i])\n",
    "        gamma_i = 1./L_i\n",
    "        \n",
    "        x_new[i] = prox_g(x[i] + gamma_i*(A[:,i].T.dot(r)), s, gamma_i)\n",
    "        \n",
    "        \n",
    "        r = r + A[:,i]*(x[i] - x_new[i]) # update r\n",
    "\n",
    "        x = x_new\n",
    "        \n",
    "        obj = 0.5 * np.linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "        err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A, b = simu_linreg(coefs)\n",
    "x0 = np.zeros(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the algorithm with the Lasso penalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lauching Coordinate Descent solver...\n",
      "   it    |   obj    |   err   \n",
      "       0 | 2.20e+03 | 1.00e+00\n",
      "      10 | 1.87e+03 | 1.12e+00\n",
      "      20 | 1.69e+03 | 1.06e+00\n",
      "      30 | 1.50e+03 | 1.12e+00\n",
      "      40 | 8.09e+02 | 8.37e-01\n",
      "      50 | 7.85e+02 | 7.81e-01\n",
      "      60 | 7.30e+02 | 7.17e-01\n",
      "      70 | 7.10e+02 | 6.94e-01\n",
      "      80 | 7.05e+02 | 6.91e-01\n",
      "      90 | 5.89e+02 | 5.44e-01\n",
      "     100 | 5.76e+02 | 5.10e-01\n",
      "     110 | 5.60e+02 | 4.85e-01\n",
      "     120 | 5.56e+02 | 4.80e-01\n",
      "     130 | 5.51e+02 | 4.70e-01\n",
      "     140 | 5.50e+02 | 4.68e-01\n",
      "     150 | 5.35e+02 | 4.30e-01\n",
      "     160 | 5.29e+02 | 4.18e-01\n",
      "     170 | 5.17e+02 | 3.81e-01\n",
      "     180 | 5.17e+02 | 3.77e-01\n",
      "     190 | 5.06e+02 | 3.39e-01\n",
      "     200 | 5.06e+02 | 3.42e-01\n",
      "     210 | 5.01e+02 | 3.23e-01\n",
      "     220 | 5.00e+02 | 3.24e-01\n",
      "     230 | 4.82e+02 | 2.76e-01\n",
      "     240 | 4.82e+02 | 2.78e-01\n",
      "     250 | 4.80e+02 | 2.78e-01\n",
      "     260 | 4.79e+02 | 2.72e-01\n",
      "     270 | 4.76e+02 | 2.68e-01\n",
      "     280 | 4.75e+02 | 2.65e-01\n",
      "     290 | 4.75e+02 | 2.66e-01\n",
      "     300 | 4.73e+02 | 2.54e-01\n",
      "     310 | 4.71e+02 | 2.35e-01\n",
      "     320 | 4.68e+02 | 2.26e-01\n",
      "     330 | 4.64e+02 | 2.05e-01\n",
      "     340 | 4.62e+02 | 2.06e-01\n",
      "     350 | 4.62e+02 | 2.05e-01\n",
      "     360 | 4.62e+02 | 2.03e-01\n",
      "     370 | 4.62e+02 | 2.04e-01\n",
      "     380 | 4.60e+02 | 1.93e-01\n",
      "     390 | 4.60e+02 | 1.91e-01\n",
      "     400 | 4.60e+02 | 1.92e-01\n",
      "     410 | 4.59e+02 | 1.90e-01\n",
      "     420 | 4.58e+02 | 1.91e-01\n",
      "     430 | 4.58e+02 | 1.87e-01\n",
      "     440 | 4.58e+02 | 1.88e-01\n",
      "     450 | 4.57e+02 | 1.86e-01\n",
      "     460 | 4.57e+02 | 1.85e-01\n",
      "     470 | 4.57e+02 | 1.84e-01\n",
      "     480 | 4.57e+02 | 1.83e-01\n",
      "     490 | 4.57e+02 | 1.83e-01\n",
      "     500 | 4.57e+02 | 1.82e-01\n",
      "     510 | 4.57e+02 | 1.82e-01\n",
      "     520 | 4.57e+02 | 1.82e-01\n",
      "     530 | 4.56e+02 | 1.81e-01\n",
      "     540 | 4.56e+02 | 1.84e-01\n",
      "     550 | 4.56e+02 | 1.84e-01\n",
      "     560 | 4.56e+02 | 1.84e-01\n",
      "     570 | 4.56e+02 | 1.84e-01\n",
      "     580 | 4.56e+02 | 1.85e-01\n",
      "     590 | 4.56e+02 | 1.86e-01\n",
      "     600 | 4.56e+02 | 1.87e-01\n",
      "     610 | 4.56e+02 | 1.87e-01\n",
      "     620 | 4.56e+02 | 1.88e-01\n",
      "     630 | 4.56e+02 | 1.89e-01\n",
      "     640 | 4.56e+02 | 1.89e-01\n",
      "     650 | 4.56e+02 | 1.89e-01\n",
      "     660 | 4.56e+02 | 1.89e-01\n",
      "     670 | 4.56e+02 | 1.89e-01\n",
      "     680 | 4.56e+02 | 1.89e-01\n",
      "     690 | 4.56e+02 | 1.90e-01\n",
      "     700 | 4.56e+02 | 1.90e-01\n",
      "     710 | 4.56e+02 | 1.90e-01\n",
      "     720 | 4.56e+02 | 1.90e-01\n",
      "     730 | 4.56e+02 | 1.91e-01\n",
      "     740 | 4.56e+02 | 1.91e-01\n",
      "     750 | 4.56e+02 | 1.91e-01\n",
      "     760 | 4.56e+02 | 1.91e-01\n",
      "     770 | 4.56e+02 | 1.91e-01\n",
      "     780 | 4.56e+02 | 1.91e-01\n",
      "     790 | 4.56e+02 | 1.91e-01\n",
      "     800 | 4.56e+02 | 1.91e-01\n",
      "     810 | 4.56e+02 | 1.91e-01\n",
      "     820 | 4.56e+02 | 1.91e-01\n",
      "     830 | 4.56e+02 | 1.91e-01\n",
      "     840 | 4.56e+02 | 1.91e-01\n",
      "     850 | 4.56e+02 | 1.91e-01\n",
      "     860 | 4.56e+02 | 1.91e-01\n",
      "     870 | 4.56e+02 | 1.91e-01\n",
      "     880 | 4.56e+02 | 1.92e-01\n",
      "     890 | 4.56e+02 | 1.92e-01\n",
      "     900 | 4.56e+02 | 1.92e-01\n",
      "     910 | 4.56e+02 | 1.92e-01\n",
      "     920 | 4.56e+02 | 1.92e-01\n",
      "     930 | 4.56e+02 | 1.92e-01\n",
      "     940 | 4.56e+02 | 1.92e-01\n",
      "     950 | 4.56e+02 | 1.92e-01\n",
      "     960 | 4.56e+02 | 1.92e-01\n",
      "     970 | 4.56e+02 | 1.92e-01\n",
      "     980 | 4.56e+02 | 1.92e-01\n",
      "     990 | 4.56e+02 | 1.92e-01\n",
      "    1000 | 4.56e+02 | 1.92e-01\n"
     ]
    }
   ],
   "source": [
    "s = 1e-2\n",
    "n_iter = 1000\n",
    "\n",
    "g = lasso\n",
    "prox_g = prox_lasso\n",
    "\n",
    "x, objectives, errors = cd_linreg(x0, A, b, g, prox_g, s=s, n_iter=n_iter, x_true=coefs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the algorithm using the indicator of $\\mathbb{R}+$ as a penalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lauching Coordinate Descent solver...\n",
      "   it    |   obj    |   err   \n",
      "       0 | 2.20e+03 | 1.00e+00\n",
      "      10 | 1.36e+03 | 9.55e-01\n",
      "      20 | 1.32e+03 | 9.46e-01\n",
      "      30 | 1.32e+03 | 9.20e-01\n",
      "      40 | 9.97e+02 | 8.70e-01\n",
      "      50 | 8.67e+02 | 8.51e-01\n",
      "      60 | 8.59e+02 | 8.52e-01\n",
      "      70 | 8.19e+02 | 8.27e-01\n",
      "      80 | 8.03e+02 | 8.12e-01\n",
      "      90 | 7.87e+02 | 8.03e-01\n",
      "     100 | 7.58e+02 | 7.51e-01\n",
      "     110 | 7.53e+02 | 7.44e-01\n",
      "     120 | 7.44e+02 | 7.25e-01\n",
      "     130 | 7.40e+02 | 7.23e-01\n",
      "     140 | 7.39e+02 | 7.22e-01\n",
      "     150 | 7.37e+02 | 7.21e-01\n",
      "     160 | 6.40e+02 | 5.85e-01\n",
      "     170 | 6.27e+02 | 5.54e-01\n",
      "     180 | 6.19e+02 | 5.58e-01\n",
      "     190 | 6.19e+02 | 5.58e-01\n",
      "     200 | 5.99e+02 | 5.56e-01\n",
      "     210 | 5.95e+02 | 5.39e-01\n",
      "     220 | 5.94e+02 | 5.39e-01\n",
      "     230 | 5.68e+02 | 4.84e-01\n",
      "     240 | 5.68e+02 | 4.76e-01\n",
      "     250 | 5.11e+02 | 3.49e-01\n",
      "     260 | 5.08e+02 | 3.42e-01\n",
      "     270 | 5.08e+02 | 3.40e-01\n",
      "     280 | 5.08e+02 | 3.40e-01\n",
      "     290 | 5.06e+02 | 3.33e-01\n",
      "     300 | 5.05e+02 | 3.26e-01\n",
      "     310 | 5.00e+02 | 3.04e-01\n",
      "     320 | 4.88e+02 | 2.40e-01\n",
      "     330 | 4.88e+02 | 2.44e-01\n",
      "     340 | 4.88e+02 | 2.44e-01\n",
      "     350 | 4.87e+02 | 2.44e-01\n",
      "     360 | 4.83e+02 | 2.21e-01\n",
      "     370 | 4.81e+02 | 2.19e-01\n",
      "     380 | 4.81e+02 | 2.19e-01\n",
      "     390 | 4.81e+02 | 2.13e-01\n",
      "     400 | 4.81e+02 | 2.12e-01\n",
      "     410 | 4.81e+02 | 2.14e-01\n",
      "     420 | 4.81e+02 | 2.12e-01\n",
      "     430 | 4.80e+02 | 2.14e-01\n",
      "     440 | 4.80e+02 | 2.15e-01\n",
      "     450 | 4.80e+02 | 2.15e-01\n",
      "     460 | 4.77e+02 | 1.97e-01\n",
      "     470 | 4.76e+02 | 1.81e-01\n",
      "     480 | 4.72e+02 | 1.43e-01\n",
      "     490 | 4.71e+02 | 1.40e-01\n",
      "     500 | 4.71e+02 | 1.43e-01\n",
      "     510 | 4.71e+02 | 1.41e-01\n",
      "     520 | 4.70e+02 | 1.36e-01\n",
      "     530 | 4.70e+02 | 1.33e-01\n",
      "     540 | 4.69e+02 | 1.24e-01\n",
      "     550 | 4.69e+02 | 1.28e-01\n",
      "     560 | 4.69e+02 | 1.29e-01\n",
      "     570 | 4.69e+02 | 1.28e-01\n",
      "     580 | 4.69e+02 | 1.29e-01\n",
      "     590 | 4.69e+02 | 1.29e-01\n",
      "     600 | 4.69e+02 | 1.20e-01\n",
      "     610 | 4.68e+02 | 1.19e-01\n",
      "     620 | 4.68e+02 | 1.19e-01\n",
      "     630 | 4.68e+02 | 1.19e-01\n",
      "     640 | 4.68e+02 | 1.17e-01\n",
      "     650 | 4.68e+02 | 1.17e-01\n",
      "     660 | 4.68e+02 | 1.17e-01\n",
      "     670 | 4.68e+02 | 1.17e-01\n",
      "     680 | 4.68e+02 | 1.17e-01\n",
      "     690 | 4.68e+02 | 1.14e-01\n",
      "     700 | 4.68e+02 | 1.14e-01\n",
      "     710 | 4.68e+02 | 1.15e-01\n",
      "     720 | 4.68e+02 | 1.14e-01\n",
      "     730 | 4.68e+02 | 1.14e-01\n",
      "     740 | 4.68e+02 | 1.14e-01\n",
      "     750 | 4.68e+02 | 1.14e-01\n",
      "     760 | 4.68e+02 | 1.13e-01\n",
      "     770 | 4.68e+02 | 1.13e-01\n",
      "     780 | 4.68e+02 | 1.14e-01\n",
      "     790 | 4.68e+02 | 1.14e-01\n",
      "     800 | 4.68e+02 | 1.14e-01\n",
      "     810 | 4.68e+02 | 1.14e-01\n",
      "     820 | 4.68e+02 | 1.14e-01\n",
      "     830 | 4.68e+02 | 1.14e-01\n",
      "     840 | 4.68e+02 | 1.14e-01\n",
      "     850 | 4.68e+02 | 1.14e-01\n",
      "     860 | 4.68e+02 | 1.14e-01\n",
      "     870 | 4.68e+02 | 1.14e-01\n",
      "     880 | 4.68e+02 | 1.14e-01\n",
      "     890 | 4.68e+02 | 1.14e-01\n",
      "     900 | 4.68e+02 | 1.14e-01\n",
      "     910 | 4.68e+02 | 1.14e-01\n",
      "     920 | 4.68e+02 | 1.14e-01\n",
      "     930 | 4.68e+02 | 1.14e-01\n",
      "     940 | 4.68e+02 | 1.14e-01\n",
      "     950 | 4.68e+02 | 1.14e-01\n",
      "     960 | 4.68e+02 | 1.14e-01\n",
      "     970 | 4.68e+02 | 1.14e-01\n",
      "     980 | 4.68e+02 | 1.14e-01\n",
      "     990 | 4.68e+02 | 1.14e-01\n",
      "    1000 | 4.68e+02 | 1.14e-01\n"
     ]
    }
   ],
   "source": [
    "s = 1e-2\n",
    "n_iter = 1000\n",
    "\n",
    "g = indicator\n",
    "prox_g = prox_indicator\n",
    "\n",
    "x, objectives, errors = cd_linreg(x0, A, b, g, prox_g, s=s, n_iter=n_iter, x_true=coefs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Question: evaluate qualitatively the convergence when varying the conditioning of the problem.\n",
    "\n",
    "To vary the conditioning of the problem we need to vary the s factor and see the influence on the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lauching Coordinate Descent solver...\n",
      "   it    |   obj    |   err   \n",
      "       0 | 1.91e+03 | 1.12e+00\n",
      "      10 | 1.77e+03 | 1.17e+00\n",
      "      20 | 1.61e+03 | 1.21e+00\n",
      "      30 | 1.56e+03 | 1.22e+00\n",
      "      40 | 1.19e+03 | 1.06e+00\n",
      "      50 | 1.15e+03 | 1.08e+00\n",
      "      60 | 8.83e+02 | 8.68e-01\n",
      "      70 | 7.77e+02 | 7.92e-01\n",
      "      80 | 6.85e+02 | 6.99e-01\n",
      "      90 | 6.79e+02 | 6.95e-01\n",
      "     100 | 6.67e+02 | 6.88e-01\n",
      "     110 | 6.67e+02 | 6.86e-01\n",
      "     120 | 6.65e+02 | 6.85e-01\n",
      "     130 | 6.04e+02 | 5.73e-01\n",
      "     140 | 5.62e+02 | 4.61e-01\n",
      "     150 | 5.60e+02 | 4.56e-01\n",
      "     160 | 5.56e+02 | 4.46e-01\n",
      "     170 | 5.49e+02 | 4.17e-01\n",
      "     180 | 5.31e+02 | 3.90e-01\n",
      "     190 | 5.08e+02 | 3.45e-01\n",
      "     200 | 5.07e+02 | 3.40e-01\n",
      "     210 | 5.07e+02 | 3.39e-01\n",
      "     220 | 4.96e+02 | 2.95e-01\n",
      "     230 | 4.96e+02 | 2.94e-01\n",
      "     240 | 4.87e+02 | 2.57e-01\n",
      "     250 | 4.85e+02 | 2.45e-01\n",
      "     260 | 4.83e+02 | 2.41e-01\n",
      "     270 | 4.81e+02 | 2.32e-01\n",
      "     280 | 4.80e+02 | 2.25e-01\n",
      "     290 | 4.75e+02 | 1.92e-01\n",
      "     300 | 4.75e+02 | 1.91e-01\n",
      "     310 | 4.74e+02 | 1.85e-01\n",
      "     320 | 4.74e+02 | 1.86e-01\n",
      "     330 | 4.74e+02 | 1.84e-01\n",
      "     340 | 4.72e+02 | 1.73e-01\n",
      "     350 | 4.71e+02 | 1.70e-01\n",
      "     360 | 4.71e+02 | 1.70e-01\n",
      "     370 | 4.71e+02 | 1.70e-01\n",
      "     380 | 4.71e+02 | 1.68e-01\n",
      "     390 | 4.70e+02 | 1.50e-01\n",
      "     400 | 4.70e+02 | 1.46e-01\n",
      "     410 | 4.69e+02 | 1.41e-01\n",
      "     420 | 4.69e+02 | 1.40e-01\n",
      "     430 | 4.69e+02 | 1.36e-01\n",
      "     440 | 4.69e+02 | 1.35e-01\n",
      "     450 | 4.69e+02 | 1.27e-01\n",
      "     460 | 4.69e+02 | 1.24e-01\n",
      "     470 | 4.69e+02 | 1.22e-01\n",
      "     480 | 4.69e+02 | 1.21e-01\n",
      "     490 | 4.68e+02 | 1.21e-01\n",
      "     500 | 4.68e+02 | 1.21e-01\n",
      "     510 | 4.68e+02 | 1.22e-01\n",
      "     520 | 4.68e+02 | 1.20e-01\n",
      "     530 | 4.68e+02 | 1.19e-01\n",
      "     540 | 4.68e+02 | 1.18e-01\n",
      "     550 | 4.68e+02 | 1.19e-01\n",
      "     560 | 4.68e+02 | 1.18e-01\n",
      "     570 | 4.68e+02 | 1.18e-01\n",
      "     580 | 4.68e+02 | 1.18e-01\n",
      "     590 | 4.68e+02 | 1.17e-01\n",
      "     600 | 4.68e+02 | 1.17e-01\n",
      "     610 | 4.68e+02 | 1.17e-01\n",
      "     620 | 4.68e+02 | 1.17e-01\n",
      "     630 | 4.68e+02 | 1.17e-01\n",
      "     640 | 4.68e+02 | 1.17e-01\n",
      "     650 | 4.68e+02 | 1.17e-01\n",
      "     660 | 4.68e+02 | 1.17e-01\n",
      "     670 | 4.68e+02 | 1.16e-01\n",
      "     680 | 4.68e+02 | 1.16e-01\n",
      "     690 | 4.68e+02 | 1.16e-01\n",
      "     700 | 4.68e+02 | 1.16e-01\n",
      "     710 | 4.68e+02 | 1.16e-01\n",
      "     720 | 4.68e+02 | 1.16e-01\n",
      "     730 | 4.68e+02 | 1.16e-01\n",
      "     740 | 4.68e+02 | 1.16e-01\n",
      "     750 | 4.68e+02 | 1.16e-01\n",
      "     760 | 4.68e+02 | 1.16e-01\n",
      "     770 | 4.68e+02 | 1.15e-01\n",
      "     780 | 4.68e+02 | 1.15e-01\n",
      "     790 | 4.68e+02 | 1.15e-01\n",
      "     800 | 4.68e+02 | 1.15e-01\n",
      "     810 | 4.68e+02 | 1.15e-01\n",
      "     820 | 4.68e+02 | 1.15e-01\n",
      "     830 | 4.68e+02 | 1.15e-01\n",
      "     840 | 4.68e+02 | 1.15e-01\n",
      "     850 | 4.68e+02 | 1.15e-01\n",
      "     860 | 4.68e+02 | 1.15e-01\n",
      "     870 | 4.68e+02 | 1.14e-01\n",
      "     880 | 4.68e+02 | 1.15e-01\n",
      "     890 | 4.68e+02 | 1.14e-01\n",
      "     900 | 4.68e+02 | 1.14e-01\n",
      "     910 | 4.68e+02 | 1.14e-01\n",
      "     920 | 4.68e+02 | 1.14e-01\n",
      "     930 | 4.68e+02 | 1.14e-01\n",
      "     940 | 4.68e+02 | 1.14e-01\n",
      "     950 | 4.68e+02 | 1.14e-01\n",
      "     960 | 4.68e+02 | 1.14e-01\n",
      "     970 | 4.68e+02 | 1.14e-01\n",
      "     980 | 4.68e+02 | 1.14e-01\n",
      "     990 | 4.68e+02 | 1.14e-01\n",
      "    1000 | 4.68e+02 | 1.14e-01\n"
     ]
    }
   ],
   "source": [
    "s = 1e10\n",
    "n_iter = 1000\n",
    "\n",
    "g = indicator\n",
    "prox_g = prox_indicator\n",
    "\n",
    "x, objectives, errors = cd_linreg(x0, A, b, g, prox_g, s=s, n_iter=n_iter, x_true=coefs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With $s = 10^{-2}$ the algorithm with the second penalty converges in roughly 480 iterations.\n",
    "\n",
    "With $s = 10^{-5}$ the same algorithm converges in roughly 860 iterations.\n",
    "\n",
    "With $s = 1$ the same algorithm converges in 370 iterations approximately.\n",
    "\n",
    "We can deduce that increasing s makes the convergence faster. If s is too small, the convergence can be very slow. However, if s is too large, the convergence is less accurate and the function might find a worse local minimum as the step is too large and not accurate enough.\n",
    "\n",
    "We can draw the same conclusions with the lasso penalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Bonus: Try to show that coordinate is much less affected by bad conditioning that proximal gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's vary the s coefficient in the algorithm from the previous lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $s = 10^{-2}$ the algorithm of coordinate descent with the lasso penalty converges in roughly 20 iterations.\n",
    "\n",
    "\n",
    "With $s = 10^{-5}$ the same algorithm converges in also 20 iterations approximately but finds a better minimum.\n",
    "\n",
    "With $s = 1$ the same algorithm converges in 10 iterations approximately but finds a worse minimum than with $s = 10^{-2}$.\n",
    "\n",
    "\n",
    "Bad conditioning on coordinate descent does not affect the rate of convergence substantially. Concerning ISTA, a $O(\\frac{1}{k})$ convergence is guaranteed. For FISTA, a $O(\\frac{1}{k^2})$ convergence is guaranteed. The only risk is to get a worse minimum with a bad conditioning."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
